{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f0fea7-5945-46a1-a069-bf4137a9ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999995/999995 [01:26<00:00, 11521.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total embeddings shape: (629762, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from src.embedding_manager import EmbeddingManager\n",
    "\n",
    "em = EmbeddingManager(path='../fasttext.wiki-new-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974783f-26e7-420d-addf-2fe881f31268",
   "metadata": {},
   "source": [
    "# Perform cleaning according to analysis, conducted in embeddings-analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a4f24f-5fbd-4938-9c24-7dfe89473dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_values(array, limits, title):\n",
    "    filtered_out_idx =  (array < limits[0]) | (array > limits[1])\n",
    "    filtered_out = array[filtered_out_idx]\n",
    "    print(f'Filtered out by {title} in {limits}: {filtered_out.shape[0]}')\n",
    "    return filtered_out_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144f0680-73ee-416d-9535-befa26e60be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out by mean per dimension in (-0.1, 0.1): 4\n",
      "Filtered out by median per dimension in (-0.1, 0.1): 3\n",
      "Filtered out by max per dimension in (0, 2): 17\n",
      "Filtered out by min per dimension in (-2, 0): 21\n",
      "Total filtered out dimensions count: 35\n"
     ]
    }
   ],
   "source": [
    "# Per dimension limits\n",
    "mean_dim_limits = (-0.1, 0.1)\n",
    "median_dim_limits = (-0.1, 0.1)\n",
    "max_dim_limits = (0, 2)\n",
    "min_dim_limits = (-2, 0)\n",
    "\n",
    "mean_values_per_dim = np.mean(em.vectors, axis=0)\n",
    "median_values_per_dim = np.median(em.vectors, axis=0)\n",
    "max_values_per_dim = np.max(em.vectors, axis=0)\n",
    "min_values_per_dim = np.min(em.vectors, axis=0)\n",
    "\n",
    "mean_dim_filtered_out_idx = filter_values(mean_values_per_dim, mean_dim_limits, 'mean per dimension')\n",
    "median_dim_filtered_out_idx = filter_values(median_values_per_dim, median_dim_limits, 'median per dimension')\n",
    "max_dim_filtered_out_idx = filter_values(max_values_per_dim, max_dim_limits, 'max per dimension')\n",
    "min_dim_filtered_out_idx = filter_values(min_values_per_dim, min_dim_limits, 'min per dimension')\n",
    "\n",
    "total_dimension_filter_idx = mean_dim_filtered_out_idx | median_dim_filtered_out_idx | max_dim_filtered_out_idx | min_dim_filtered_out_idx\n",
    "print(f'Total filtered out dimensions count: {em.vectors[0][total_dimension_filter_idx].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a126259-e469-4dff-9433-ff2431cd61d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629762, 265)\n",
      "(629762, 256)\n"
     ]
    }
   ],
   "source": [
    "em.vectors = em.vectors.swapaxes(0, 1)[~total_dimension_filter_idx].swapaxes(0, 1)\n",
    "print(em.vectors.shape)\n",
    "\n",
    "# reduce dimensions a bit more\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=256)\n",
    "em.vectors = pca.fit_transform(em.vectors)\n",
    "print(em.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42674271-7a60-4264-83fa-9cc3f572a0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out by mean in (-0.03, 0.03): 201\n",
      "Filtered out by median in (-0.04, 0.04): 105\n",
      "Filtered out by max in (0, 1): 67\n",
      "Filtered out by min in (-1, 0): 38\n",
      "Total filtered out entries count: 226\n",
      "['rivera' 'tulsa' 'uniting' 'في' 'docking' 'psychopathic' 'workhouse'\n",
      " 'rezko' 'olya' 'aacn' 'arpaio' 'micron' 'transducers' 'serait' 'nauvoo'\n",
      " 'sofla' 'valea' 'roxana' 'taliesin' 'vha' 'gosnell' 'disestablishments'\n",
      " 'alwyn' 'motorhead' 'dunhill' 'limehouse' 'hoodoo' 'rankersbo'\n",
      " 'michaelmas' 'epd' 'ihre' 'bruegel' 'seamount' 'keener' 'pratyya'\n",
      " 'molise' 'rovio' 'atrophied' 'vaslui' 'sogdian' 'mairead' 'bostic'\n",
      " 'adaag' 'wurzelbacher' 'almshouses' 'ahwaz' 'sobotka' 'jeremias' 'krasny'\n",
      " 'timurid' 'kamau' 'extemporaneous' 'adlai' 'dawei' 'groveling' 'novák'\n",
      " 'slatkin' 'rrt' '2606' 'riya' 'rouses' 'monotonically' 'lawfulness'\n",
      " 'municipio' 'hicham' 'gessler' 'sakari' 'sobered' 'hamley' 'impassively'\n",
      " 'hưng' 'stian' 'skelos' 'outremont' 'qic' 'arnolfini' 'aruch' 'corradini'\n",
      " 'mainzer' 'lubomir' 'blatz' 'bgb' 'helford' 'peadar' 'caddyshack'\n",
      " 'jaimes' 'katydid' 'repack' '2276' 'qalawun' 'ivm' 'arvand' 'pólya'\n",
      " 'sirat' 'effacement' 'shawbury' 'iren' 'petaholmes' 'dragoș' 'frede'\n",
      " 'celona' 'parthenius' 'parmley' 'stormbay' 'dabashi' 'pineland' 'manito'\n",
      " 'satpathy' 'natalio' 'höfer' 'pledgers' 'cartulary' 'huashan'\n",
      " 'chmielnicki' 'latihan' 'ulmann' 'identidad' 'colchagua' 'stillson'\n",
      " 'fameux' 'blifil' 'midford' 'petersburgh' '3745' 'jeli' 'kulan' 'rudloff'\n",
      " 'witcombe' 'lontano' 'cij' 'armeniapedia' 'negativ' 'sagasta' '8181'\n",
      " 'moshannon' 'slavoljub' 'pomponia' 'submarino' 'geomantic' 'qhr'\n",
      " 'herrerasaurus' 'barcott' 'dooryard' 'kulturbund' 'schmiedlová'\n",
      " 'desaturate' 'godmersham' 'workstudy' 'gomarsall' 'prearrangement'\n",
      " 'jarolím' 'mccoo' 'dòmhnallach' 'fotakis' 'luckinbill' 'krest' 'drikung'\n",
      " 'slegers' 'jujubee' 'soutpansberg' 'vrea' 'بیان' 'ferrington' 'thok'\n",
      " 'netsilik' 'ibou' 'الساعة' 'franqui' 'hornlike' 'bomc' 'mbuyi' 'merlimau'\n",
      " 'kumaresh' 'hoeffler' 'falleni' 'unretracted' 'andain' 'ridah' 'kalafina'\n",
      " 'tendence' 'pattishall' 'pericos' 'boğazköy' 'vandenburgh'\n",
      " 'letheringsett' '9474' 'تصور' '8281' 'docken' 'amerada' 'althen'\n",
      " 'gurirab' 'donici' 'litunga' 'faiez' 'iberomesornix' 'chymical'\n",
      " 'tuheitia' 'aromal' 'abercraf' 'doktorski' 'shân' 'eleousa' 'fratercula'\n",
      " 'lysbakken' 'rotblatt' 'friezland' 'mirumo' 'brandhorst' 'sadhya'\n",
      " 'goombungee' 'sosthène' 'awsat' 'aqoon' 'satolli' 'superp' 'orano'\n",
      " 'bilba' 'mortua' 'decarbonylation' 'cobl' 'dottydotdot' 'koerper' 'teleo'\n",
      " 'égoïste' 'domestications']\n"
     ]
    }
   ],
   "source": [
    "# Per entry limits\n",
    "mean_limits = (-0.03, 0.03)\n",
    "median_limits = (-0.04, 0.04)\n",
    "max_limits = (0, 1)\n",
    "min_limits = (-1, 0)\n",
    "\n",
    "mean_values = np.mean(em.vectors, axis=1)\n",
    "median_values = np.median(em.vectors, axis=1)\n",
    "max_values = np.max(em.vectors, axis=1)\n",
    "min_values = np.min(em.vectors, axis=1)\n",
    "\n",
    "mean_filtered_out_idx = filter_values(mean_values, mean_limits, 'mean')\n",
    "median_filtered_out_idx = filter_values(median_values, median_limits, 'median')\n",
    "max_filtered_out_idx = filter_values(max_values, max_limits, 'max')\n",
    "min_filtered_out_idx = filter_values(min_values, min_limits, 'min')\n",
    "\n",
    "total_entry_filter_idx = mean_filtered_out_idx | median_filtered_out_idx | max_filtered_out_idx | min_filtered_out_idx\n",
    "print(f'Total filtered out entries count: {em.vectors[total_entry_filter_idx].shape[0]}')\n",
    "print(em.words[total_entry_filter_idx])\n",
    "\n",
    "em.vectors = em.vectors[~total_entry_filter_idx]\n",
    "em.words = em.words[~total_entry_filter_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b84e7b-cf89-4deb-ad3e-f28083b6d6f9",
   "metadata": {},
   "source": [
    "## Save filtered embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69112852-e609-41d0-94a1-b186b391ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../fasttext.wiki-news-cleaned-256d.vec', 'a', encoding='UTF-8') as vec_file:\n",
    "    for i, word in enumerate(em.words):\n",
    "        vector = em.vectors[i]\n",
    "        row = word + ' ' + ' '.join([str(val) for val in vector])\n",
    "        vec_file.write(row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4db42d-eea3-4d17-8ed7-aafe2dcaa0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
